{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b146b8d2",
   "metadata": {},
   "source": [
    "# ðŸ›°ï¸ Satellite Image Super-Resolution - Inference Demo\n",
    "\n",
    "**Purpose**: Run inference on a test image using our trained SwinIR model\n",
    "\n",
    "**What this notebook does**:\n",
    "1. Sets up the environment on Google Colab\n",
    "2. Downloads the pre-trained model checkpoint\n",
    "3. Loads a test image\n",
    "4. Performs 4x super-resolution\n",
    "5. Displays and saves the results\n",
    "\n",
    "**Runtime**: ~2-5 minutes on Colab (with GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82920dd",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37880d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ Running on CPU (slower). For GPU: Runtime â†’ Change runtime type â†’ T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f985cd4",
   "metadata": {},
   "source": [
    "## Step 2: Clone the Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805f4b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the project repository\n",
    "!git clone https://github.com/Elkrisent/klymo-ascent-1.0.git\n",
    "%cd klymo-ascent-1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5c0b08",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41811451",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision pillow numpy matplotlib rasterio scikit-image\n",
    "print(\"âœ… Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d19e60",
   "metadata": {},
   "source": [
    "## Step 4: Download Pre-trained Model\n",
    "\n",
    "**Note**: The model checkpoint should be available in the repository. If not, you can upload it manually or download from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1fb903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if checkpoint exists\n",
    "checkpoint_path = \"checkpoints/swinir_best.pth\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"âœ… Model checkpoint found: {checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Checkpoint not found at {checkpoint_path}\")\n",
    "    print(\"You can:\")\n",
    "    print(\"  1. Upload swinir_best.pth to the checkpoints/ folder\")\n",
    "    print(\"  2. Or download from Google Drive (if you have the link)\")\n",
    "    \n",
    "    # Uncomment and modify if you have a Google Drive link:\n",
    "    # from google.colab import drive\n",
    "    # drive.mount('/content/drive')\n",
    "    # !cp /content/drive/MyDrive/path/to/swinir_best.pth checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a558777e",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Test Image\n",
    "\n",
    "You can either:\n",
    "- **Option A**: Use the included test image from the repository\n",
    "- **Option B**: Upload your own satellite image (GeoTIFF or PNG/JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f87a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Use test image from repository (if available)\n",
    "test_image_path = \"data/mystery_location/mystery_sr.tif\"\n",
    "\n",
    "# Option B: Upload your own image\n",
    "# Uncomment the following lines to upload:\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# test_image_path = list(uploaded.keys())[0]\n",
    "\n",
    "print(f\"Test image: {test_image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d11ad",
   "metadata": {},
   "source": [
    "## Step 6: Load and Display the Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f419bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    # Try loading as GeoTIFF\n",
    "    import rasterio\n",
    "    from rasterio.plot import reshape_as_image\n",
    "    \n",
    "    with rasterio.open(test_image_path) as src:\n",
    "        img = src.read()\n",
    "        img = reshape_as_image(img)\n",
    "        print(f\"Loaded GeoTIFF: {img.shape}, dtype: {img.dtype}\")\n",
    "        \n",
    "        # Normalize to 8-bit if needed\n",
    "        if img.dtype == np.uint16:\n",
    "            img = (img.astype(float) / 65535 * 255).astype(np.uint8)\n",
    "        \n",
    "        lr_image = img\n",
    "except:\n",
    "    # Load as regular image\n",
    "    lr_image = np.array(Image.open(test_image_path))\n",
    "    print(f\"Loaded image: {lr_image.shape}\")\n",
    "\n",
    "# Ensure RGB format\n",
    "if lr_image.ndim == 2:\n",
    "    lr_image = np.stack([lr_image]*3, axis=-1)\n",
    "elif lr_image.shape[2] > 3:\n",
    "    lr_image = lr_image[:, :, :3]\n",
    "\n",
    "print(f\"Input image shape: {lr_image.shape}\")\n",
    "\n",
    "# Display input\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(lr_image)\n",
    "plt.title(\"Input Low-Resolution Image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c0964",
   "metadata": {},
   "source": [
    "## Step 7: Load the SwinIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97b06a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from models.swinir_satellite import SatelliteSwinIR\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = SatelliteSwinIR(scale=4).to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "state_dict = checkpoint.get('model_state_dict', checkpoint)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "# Display model info\n",
    "if 'psnr' in checkpoint:\n",
    "    print(f\"âœ… Model loaded successfully! (Validation PSNR: {checkpoint['psnr']:.2f} dB)\")\n",
    "else:\n",
    "    print(\"âœ… Model loaded successfully!\")\n",
    "    \n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a232eee9",
   "metadata": {},
   "source": [
    "## Step 8: Run Inference\n",
    "\n",
    "We'll process the image in tiles if it's large to avoid memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a17f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_single_patch(lr_image, model, device):\n",
    "    \"\"\"Run inference on a single patch\"\"\"\n",
    "    # Convert to tensor\n",
    "    lr_tensor = torch.from_numpy(lr_image).permute(2, 0, 1).float() / 255.0\n",
    "    lr_tensor = lr_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        sr_tensor = model(lr_tensor)\n",
    "    \n",
    "    # Convert back to numpy\n",
    "    sr_image = sr_tensor.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
    "    sr_image = np.clip(sr_image * 255.0, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return sr_image\n",
    "\n",
    "\n",
    "def tile_inference(large_lr_image, model, device, tile_size=128, overlap=16):\n",
    "    \"\"\"Process large image by tiling with overlap\"\"\"\n",
    "    h, w, c = large_lr_image.shape\n",
    "    scale = 4\n",
    "    \n",
    "    sr_h, sr_w = h * scale, w * scale\n",
    "    sr_canvas = np.zeros((sr_h, sr_w, c), dtype=np.float32)\n",
    "    weight_map = np.zeros((sr_h, sr_w), dtype=np.float32)\n",
    "    \n",
    "    stride = tile_size - overlap\n",
    "    print(f\"Processing {h}x{w} image in {tile_size}x{tile_size} tiles...\")\n",
    "    \n",
    "    tile_count = 0\n",
    "    for y in range(0, h - tile_size + 1, stride):\n",
    "        for x in range(0, w - tile_size + 1, stride):\n",
    "            lr_tile = large_lr_image[y:y+tile_size, x:x+tile_size, :]\n",
    "            sr_tile = inference_single_patch(lr_tile, model, device)\n",
    "            \n",
    "            # Calculate SR coordinates\n",
    "            y_sr, x_sr = y * scale, x * scale\n",
    "            tile_h_sr, tile_w_sr = tile_size * scale, tile_size * scale\n",
    "            \n",
    "            # Create weight for smooth blending\n",
    "            weight = np.ones((tile_h_sr, tile_w_sr), dtype=np.float32)\n",
    "            if overlap > 0:\n",
    "                fade_size = overlap * scale\n",
    "                fade = np.linspace(0, 1, fade_size, dtype=np.float32)\n",
    "                weight[:fade_size, :] *= fade[:, np.newaxis]\n",
    "                weight[-fade_size:, :] *= fade[::-1, np.newaxis]\n",
    "                weight[:, :fade_size] *= fade[np.newaxis, :]\n",
    "                weight[:, -fade_size:] *= fade[::-1][np.newaxis, :]\n",
    "            \n",
    "            # Accumulate\n",
    "            sr_canvas[y_sr:y_sr+tile_h_sr, x_sr:x_sr+tile_w_sr, :] += (\n",
    "                sr_tile.astype(np.float32) * weight[:, :, np.newaxis]\n",
    "            )\n",
    "            weight_map[y_sr:y_sr+tile_h_sr, x_sr:x_sr+tile_w_sr] += weight\n",
    "            \n",
    "            tile_count += 1\n",
    "            if tile_count % 10 == 0:\n",
    "                print(f\"  Processed {tile_count} tiles...\")\n",
    "    \n",
    "    # Normalize by weights\n",
    "    sr_canvas = sr_canvas / (weight_map[:, :, np.newaxis] + 1e-8)\n",
    "    sr_canvas = np.clip(sr_canvas, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    print(f\"âœ… Tiling complete ({tile_count} tiles)\")\n",
    "    return sr_canvas\n",
    "\n",
    "\n",
    "# Decide whether to use tiling\n",
    "h, w = lr_image.shape[:2]\n",
    "if h <= 256 and w <= 256:\n",
    "    # Small image - process directly\n",
    "    print(\"Processing image directly (no tiling needed)...\")\n",
    "    sr_image = inference_single_patch(lr_image, model, device)\n",
    "else:\n",
    "    # Large image - use tiling\n",
    "    sr_image = tile_inference(lr_image, model, device, tile_size=128, overlap=16)\n",
    "\n",
    "print(f\"âœ… Super-resolution complete!\")\n",
    "print(f\"Output shape: {sr_image.shape} (4x upscale)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe47d39",
   "metadata": {},
   "source": [
    "## Step 9: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d3c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display side-by-side comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "axes[0].imshow(lr_image)\n",
    "axes[0].set_title(f\"Input LR Image\\n{lr_image.shape[0]}Ã—{lr_image.shape[1]}\", fontsize=16)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(sr_image)\n",
    "axes[1].set_title(f\"Output SR Image (4x)\\n{sr_image.shape[0]}Ã—{sr_image.shape[1]}\", fontsize=16)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8410f",
   "metadata": {},
   "source": [
    "## Step 10: Display Detail Comparison\n",
    "\n",
    "Let's zoom into a specific region to see the quality improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e05fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a patch for comparison\n",
    "patch_size = 64\n",
    "lr_patch = lr_image[:patch_size, :patch_size]\n",
    "sr_patch = sr_image[:patch_size*4, :patch_size*4]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "axes[0].imshow(lr_patch)\n",
    "axes[0].set_title(f\"LR Patch ({patch_size}Ã—{patch_size})\", fontsize=14)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(sr_patch)\n",
    "axes[1].set_title(f\"SR Patch ({patch_size*4}Ã—{patch_size*4})\", fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62936025",
   "metadata": {},
   "source": [
    "## Step 11: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1168a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SR image\n",
    "output_path = \"sr_output.png\"\n",
    "Image.fromarray(sr_image).save(output_path)\n",
    "print(f\"âœ… Saved super-resolved image to: {output_path}\")\n",
    "\n",
    "# Download the result\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(output_path)\n",
    "    print(\"âœ… File download started\")\n",
    "except:\n",
    "    print(\"(Download manually or use Colab files panel)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda07e3f",
   "metadata": {},
   "source": [
    "## Step 12: Quality Metrics (Optional)\n",
    "\n",
    "If you have a high-resolution ground truth image, you can compute PSNR and SSIM metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Compute metrics if you have ground truth\n",
    "# Uncomment if you have an HR ground truth image:\n",
    "\n",
    "# from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "# from skimage.metrics import structural_similarity as ssim\n",
    "# \n",
    "# # Load ground truth\n",
    "# hr_image = np.array(Image.open(\"path_to_ground_truth.png\"))\n",
    "# \n",
    "# # Compute metrics\n",
    "# psnr_value = psnr(hr_image, sr_image)\n",
    "# ssim_value = ssim(hr_image, sr_image, channel_axis=2, data_range=255)\n",
    "# \n",
    "# print(f\"PSNR: {psnr_value:.2f} dB\")\n",
    "# print(f\"SSIM: {ssim_value:.4f}\")\n",
    "\n",
    "print(\"âœ… Inference demo complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110004ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "- âœ… Loading a pre-trained SwinIR model\n",
    "- âœ… Processing satellite images with 4x super-resolution\n",
    "- âœ… Handling large images with tiled inference\n",
    "- âœ… Visualizing and saving results\n",
    "\n",
    "**Model Details**:\n",
    "- Architecture: SwinIR (Swin Transformer for Image Restoration)\n",
    "- Scale: 4x upsampling\n",
    "- Input: Low-resolution satellite imagery (RGB)\n",
    "- Output: High-resolution satellite imagery (4Ã— larger)\n",
    "\n",
    "**Use Cases**:\n",
    "- Enhancing satellite imagery resolution\n",
    "- Improving detail in Earth observation data\n",
    "- Supporting analysis tasks requiring higher spatial resolution"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
